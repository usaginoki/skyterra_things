hidden_layer_sizes: (200,)
activation: tanh
alpha: 0.0001
learning_rate_init: 0.001
dropout: 0.1
